adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-08
adam_weight_decay: 0.0001
adam_weight_decay_text_encoder: 0.001
allow_tf32: false
cache_latents: false
checkpointing_steps: 1000
checkpoints_total_limit: null
dataloader_num_workers: 16
gradient_accumulation_steps: 4
gradient_checkpointing: false
guidance_scale: 3.5
learning_rate: 1.0e-05
local_rank: 0
logging_dir: logs
lora_alpha: 512
lora_dropout: 0.0
lora_layers: to_q, to_k, to_v, to_out.0
lr_num_cycles: 1
lr_power: 1.0
lr_scheduler: constant
lr_warmup_steps: 500
max_grad_norm: 1.0
max_sequence_length: 512
max_train_steps: 45400
mixed_precision: bf16
num_train_epochs: 100
optimizer: AdamW
output_dir: flux_inversion_lora_target_0.3_512
pretrained_model_name_or_path: black-forest-labs/FLUX.1-dev
prodigy_beta3: null
prodigy_decouple: true
prodigy_safeguard_warmup: true
prodigy_use_bias_correction: true
rank: 512
report_to: tensorboard
resolution: 1024
resume_from_checkpoint: null
seed: null
target_timestep: 0.3
text_encoder_lr: 5.0e-06
train_batch_size: 1
upcast_before_saving: false
use_8bit_adam: false
weighting_scheme: none
